<!-- Observation 2: The Planet Can't Afford This -->
{% include observations/observation-detail.html
    id="observation-2"
    number="02"
    title="The Planet Can't Afford This"
    bg="bg-terminal-surface"
    problem="Training large language models consumes extraordinary amounts of energy. Training GPT-3 produced an estimated 502 tonnes of CO2. Every ChatGPT query uses roughly 15 times more energy than a Google search. Inference now accounts for more than half of total carbon footprint."
    quote="Electricity demand from data centres worldwide is set to more than double by 2030 to around 945 terawatt-hours (TWh), slightly more than the entire electricity consumption of Japan today. AI will be the most significant driver of this increase, with electricity demand from AI-optimised data centres projected to more than quadruple by 2030. In the United States, power consumption by data centres is on course to account for almost half of the growth in electricity demand between now and 2030."
    quote_source="<a href='https://www.iea.org/news/ai-is-set-to-drive-surging-electricity-demand-from-data-centres-while-offering-the-potential-to-transform-how-the-energy-sector-works' target='_blank' class='text-terminal-green hover:underline'>IEA Energy and AI Report (2025)</a>"
    why_matters="We love AI. We think it can do tremendous good. But we refuse to ignore the costs. Climate change is real. Energy consumption matters. And 'move fast and break things' isn't a philosophy that scales to planetary systems."
    ohgod_solution="OHGOD!! What if we could build efficiency into these foundation models? Training on a single GPU in 24 hours under someone's desk might mean people can take care of their inference needs right from home, with their own electricity."
%}
